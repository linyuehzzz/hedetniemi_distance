{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "all_pair_distance_cuda.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN5SkY+ePNz1JMvX3EBP/Vp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linyuehzzz/hedetniemi_distance/blob/master/all_pair_distance_cuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrTaIelAvUyh",
        "colab_type": "text"
      },
      "source": [
        "##**CUDA for all-pair distance algorithms**\n",
        "CUDA parallelism for all-pair distance algorithms.  \n",
        "Yue Lin (lin.3326 at osu.edu)  \n",
        "Created: 6/12/2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "083OUoAWPj2J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "b62d0293-a6e0-4d88-9c41-fe6a5b11ce15"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rya-4S2c_mPy"
      },
      "source": [
        "#### **Install packages** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7Au10ccQPam",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "3c482a03-0807-439d-9829-eb050f59d990"
      },
      "source": [
        "!pip install timeout-decorator"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting timeout-decorator\n",
            "  Downloading https://files.pythonhosted.org/packages/07/1c/0d9adcb848f1690f3253dcb1c1557b6cf229a93e724977cb83f266cbd0ae/timeout-decorator-0.4.1.tar.gz\n",
            "Building wheels for collected packages: timeout-decorator\n",
            "  Building wheel for timeout-decorator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for timeout-decorator: filename=timeout_decorator-0.4.1-cp36-none-any.whl size=5021 sha256=121582383f71f6667722e6b6aa1fe65533da523314b4691f989139a270e80c65\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/e6/ea/7387e3629cb46ba65140141f972745b823f4486c6fe884ccb8\n",
            "Successfully built timeout-decorator\n",
            "Installing collected packages: timeout-decorator\n",
            "Successfully installed timeout-decorator-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMGLlynLRyMO",
        "colab_type": "text"
      },
      "source": [
        "#### **CUDA device query** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oBHXLhsRfPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "c129ae00-2831-45d0-ad93-e2a18f151b0d"
      },
      "source": [
        "!nvcc --version\n",
        "from numba import cuda\n",
        "print(cuda.gpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "<Managed Device 0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnjDI4fbSGHm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cfce08ae-fbf0-41fa-a635-57d4f2e083d2"
      },
      "source": [
        "%cd /usr/local/cuda-10.1/samples/1_Utilities/deviceQuery\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/cuda-10.1/samples/1_Utilities/deviceQuery\n",
            "deviceQuery.cpp  Makefile  NsightEclipse.xml  readme.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOm7s7mBSRCg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "outputId": "d59f0379-58f6-45db-b73e-f0f9005556aa"
      },
      "source": [
        "!make\n",
        "!./deviceQuery"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make: Nothing to be done for 'all'.\n",
            "./deviceQuery Starting...\n",
            "\n",
            " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"Tesla P100-PCIE-16GB\"\n",
            "  CUDA Driver Version / Runtime Version          10.1 / 10.1\n",
            "  CUDA Capability Major/Minor version number:    6.0\n",
            "  Total amount of global memory:                 16281 MBytes (17071734784 bytes)\n",
            "  (56) Multiprocessors, ( 64) CUDA Cores/MP:     3584 CUDA Cores\n",
            "  GPU Max Clock rate:                            1329 MHz (1.33 GHz)\n",
            "  Memory Clock rate:                             715 Mhz\n",
            "  Memory Bus Width:                              4096-bit\n",
            "  L2 Cache Size:                                 4194304 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  2048\n",
            "  Maximum number of threads per block:           1024\n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated GPU sharing Host Memory:            No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Alignment requirement for Surfaces:            Yes\n",
            "  Device has ECC support:                        Enabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device supports Compute Preemption:            Yes\n",
            "  Supports Cooperative Kernel Launch:            Yes\n",
            "  Supports MultiDevice Co-op Kernel Launch:      Yes\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n",
            "  Compute Mode:\n",
            "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
            "\n",
            "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.1, NumDevs = 1\n",
            "Result = PASS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZhcIrCevlc4",
        "colab_type": "text"
      },
      "source": [
        "#### **Read graph data** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw69kvqGGpUL",
        "colab_type": "text"
      },
      "source": [
        "##### Data from the original article"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qev-12ZOGlco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## [node i, node j, distance between node i and j]\n",
        "## using data from example 1: San Francisco Bay Area Graph of Time-Distances (in minutes)\n",
        "data = [[1, 2, 30], [1, 4, 30], [1, 9, 40],\n",
        "        [2, 3, 25], [2, 4, 40], [3, 4, 50],\n",
        "        [4, 5, 30], [4, 6, 20], [5, 7, 25],\n",
        "        [6, 7, 20], [6, 9, 20], [7, 8, 25],\n",
        "        [8, 9, 20]]\n",
        "nodes = 9"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMaJB5Y1GmMQ",
        "colab_type": "text"
      },
      "source": [
        "##### Read random graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ej6mBJFQd3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "e0928057-d4cf-42a7-b7fb-fa6b090b7933"
      },
      "source": [
        "%cd '/content/gdrive/My Drive/Colab Notebooks/hedetniemi_matrix_sum'\n",
        "\n",
        "## Number of nodes (100/1,000/10,000/100,000/1,000,000)\n",
        "nodes = 1000\n",
        "print('Nodes: ', nodes)\n",
        "## Total degree\n",
        "degree = 3\n",
        "print('Degree: ', degree)\n",
        "\n",
        "data = []\n",
        "with open('graph_n' + str(nodes) + '_d' + str(degree) + '.txt', 'r') as f:\n",
        "  lines = f.read().splitlines()\n",
        "  for line in lines:\n",
        "    l = line.split()\n",
        "    item = [int(l[0]), int(l[1]), float(l[2])]\n",
        "    data.append(item)\n",
        "\n",
        "print(data[0])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/hedetniemi_matrix_sum\n",
            "Nodes:  1000\n",
            "Degree:  3\n",
            "[609, 621, 18.019071417527243]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cvAjdVsDR1FL"
      },
      "source": [
        "#### **Configure CUDA** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dpUmV2deRwDt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "ef2b0e3f-6817-4dfd-af06-dcb3adeb093f"
      },
      "source": [
        "import math\n",
        "\n",
        "# number of streams\n",
        "NUM_STREAMS = 5\n",
        "# number of threads per block: 32、128、256\n",
        "NUM_THREADS = 32\n",
        "\n",
        "def get_cuda_execution_config(n):\n",
        "  numStream = NUM_STREAMS\n",
        "  numSegment = n // numStream\n",
        "  dimBlock = (NUM_THREADS, NUM_THREADS)\n",
        "  dimGrid = (math.ceil(numSegment / NUM_THREADS), math.ceil(numSegment / NUM_THREADS))\n",
        "\n",
        "  return dimGrid, dimBlock, numStream, numSegment\n",
        "\n",
        "\n",
        "dimGrid, dimBlock, numStream, numSegment = get_cuda_execution_config(nodes)\n",
        "print('numStream: ', numStream)\n",
        "print('numSegment: ', numSegment)\n",
        "print('dimGrid: ', dimGrid)\n",
        "print('dimBlock: ', dimBlock)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numStream:  5\n",
            "numSegment:  200\n",
            "dimGrid:  (7, 7)\n",
            "dimBlock:  (32, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8xMYn_Pyrk0",
        "colab_type": "text"
      },
      "source": [
        "#### **Hedetniemi distance** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93Y_nhjy8g8P",
        "colab_type": "text"
      },
      "source": [
        "##### Construct distance matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mX7kwBz8gPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "7d8e65a4-d3b7-4b36-838e-db614d8a0838"
      },
      "source": [
        "from timeit import default_timer\n",
        "from numba import cuda, njit\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def graph2dist(graph, dist_mtx, n):\n",
        "  stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "\n",
        "  ## initialize distance matrix\n",
        "  x, y = cuda.grid(2)\n",
        "  for i in range(x, n, stride):\n",
        "    for j in range(y, n, stride):\n",
        "      dist_mtx[i,j] = np.inf\n",
        "\n",
        "  ## calculate distance matrix\n",
        "  x = cuda.grid(1)\n",
        "  for i in range(x, graph.shape[0], stride):\n",
        "    a = int(graph[i,0]) - 1\n",
        "    b = int(graph[i,1]) - 1\n",
        "    d = graph[i,2]\n",
        "    dist_mtx[a,b] = d\n",
        "    dist_mtx[b,a] = d\n",
        "  \n",
        "  ## set diagonal to 0\n",
        "  y = cuda.grid(1)\n",
        "  if y < n:\n",
        "    dist_mtx[y,y] = 0.0\n",
        "\n",
        "\n",
        "def distance_matrix(graph, n):\n",
        "  ## copy data to device\n",
        "  graph_device = cuda.to_device(graph)\n",
        "  dist_mtx_device = cuda.device_array(shape=(n,n))\n",
        "\n",
        "  ## calculate distance matrix\n",
        "  graph2dist[dimGrid, dimBlock](graph_device, dist_mtx_device, n)\n",
        "  \n",
        "  ## copy data to host\n",
        "  dist_mtx_host = dist_mtx_device.copy_to_host()\n",
        " \n",
        "  return dist_mtx_host\n",
        "\n",
        "\n",
        "## print time costs\n",
        "try:\n",
        "  start = default_timer()\n",
        "  dist_mtx = distance_matrix(np.array(data), nodes)\n",
        "  print(dist_mtx)\n",
        "  stop = default_timer()\n",
        "  print('Time: ', stop - start)\n",
        "except:\n",
        "  print('Time: inf')\n",
        "  raise"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0. 30. inf 30. inf inf inf inf 40.]\n",
            " [30.  0. 25. 40. inf inf inf inf inf]\n",
            " [inf 25.  0. 50. inf inf inf inf inf]\n",
            " [30. 40. 50.  0. 30. 20. inf inf inf]\n",
            " [inf inf inf 30.  0. inf 25. inf inf]\n",
            " [inf inf inf 20. inf  0. 20. inf 20.]\n",
            " [inf inf inf inf 25. 20.  0. 25. inf]\n",
            " [inf inf inf inf inf inf 25.  0. 20.]\n",
            " [40. inf inf inf inf 20. inf 20.  0.]]\n",
            "Time:  0.2706781830002001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_8aCYV6pFnX0"
      },
      "source": [
        "##### Calculate Hedetniemi Matrix Sum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwMScYG8LRSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "f6d83587-7280-4a26-cb3a-9c3f5f90725c"
      },
      "source": [
        "from timeit import default_timer\n",
        "from numba import cuda, njit, float32\n",
        "from operator import *\n",
        "import numpy as np\n",
        "\n",
        "@cuda.jit\n",
        "def init_mtx(matrix, mtx_a_t_1, mtx_a_t, n):\n",
        "  # initialize distance matrix\n",
        "  x, y = cuda.grid(2)\n",
        "  if x < n and y < n:\n",
        "    mtx_a_t[x,y] = np.inf\n",
        "    mtx_a_t_1[x,y] = matrix[x,y]\n",
        "\n",
        "@cuda.jit\n",
        "def cal_mtx(matrix, mtx_a_t_1, mtx_a_t, n):\n",
        "  # bpg = cuda.gridDim.x\n",
        "  # tpb = cuda.blockDim.x\n",
        "\n",
        "  # stride = cuda.gridDim.x * cuda.blockDim.x  \n",
        "  # tx = cuda.threadIdx.x\n",
        "  # ty = cuda.threadIdx.y\n",
        "  # sA = cuda.shared.array(shape=(NUM_THREADS, NUM_THREADS), dtype=float32)\n",
        "  # sB = cuda.shared.array(shape=(NUM_THREADS, NUM_THREADS), dtype=float32)\n",
        "\n",
        "  # calculate hedetniemi matrix sum\n",
        "  x, y = cuda.grid(2)\n",
        "  if x < n and y < n:\n",
        "    summ = np.inf\n",
        "    z = cuda.grid(1)\n",
        "    if z < n:\n",
        "      summ = min(summ, mtx_a_t_1[x, z] + matrix[z, y])\n",
        "    mtx_a_t[x,y] = summ\n",
        "    # summ = np.inf\n",
        "    # for i in range(bpg):\n",
        "    #   sA[tx, ty] = mtx_a_t_1[x, ty + i * tpb]\n",
        "    #   sB[tx, ty] = matrix[tx + i * tpb, y]\n",
        "    #   cuda.syncthreads()\n",
        "    #   for j in range(tpb):\n",
        "    #     summ = min(summ, sA[tx, j] + sB[j, ty])\n",
        "    #     cuda.syncthreads()\n",
        "    # mtx_a_t[x,y] = summ\n",
        "\n",
        "  # x, y = cuda.grid(2)\n",
        "  # if x < n and y < n:\n",
        "  #   mtx_a_t_1[x,y] = mtx_a_t[x,y]    \n",
        "\n",
        "\n",
        "def hede_distance(matrix, n):\n",
        "  ## copy data to device\n",
        "  matrix_device = cuda.to_device(matrix)\n",
        "  mtx_a_t_1_device = cuda.device_array(shape=(n,n))\n",
        "  mtx_a_t_device = cuda.device_array(shape=(n,n))\n",
        "\n",
        "  ## initialize hedetniemi distance\n",
        "  init_mtx[dimGrid, dimBlock](matrix_device, mtx_a_t_1_device, mtx_a_t_device, n)\n",
        "\n",
        "  ## calculate hedetniemi distance\n",
        "  # for p in range(n):\n",
        "  cal_mtx[dimGrid, dimBlock](matrix_device, mtx_a_t_1_device, mtx_a_t_device, n)\n",
        "  print(mtx_a_t_1_device.copy_to_host()[0])\n",
        "  print(mtx_a_t_device.copy_to_host()[0])\n",
        "  cal_mtx[dimGrid, dimBlock](matrix_device, mtx_a_t_1_device, mtx_a_t_device, n)\n",
        "  print(mtx_a_t_1_device.copy_to_host()[0])\n",
        "  print(mtx_a_t_device.copy_to_host()[0])\n",
        "  \n",
        "  ## copy data to host\n",
        "  mtx_a_t_host = mtx_a_t_device.copy_to_host()\n",
        " \n",
        "  return mtx_a_t_host\n",
        "\n",
        "\n",
        "## print time costs\n",
        "try:\n",
        "  start = default_timer()\n",
        "  mtx_a_t = hede_distance(dist_mtx, nodes)\n",
        "  print(mtx_a_t)\n",
        "  stop = default_timer()\n",
        "  print('Time: ', stop - start)\n",
        "except:\n",
        "  print('Time: inf')\n",
        "  raise\n",
        "\n",
        "## print shortest path matrix\n",
        "with open('hedet_mtx_nb_cuda.txt', 'w') as fw:\n",
        "  fw.write('\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in mtx_a_t.tolist()]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0. 30. inf 30. inf inf inf inf 40.]\n",
            "[ 0. 30. inf 30. inf inf inf inf 40.]\n",
            "[ 0. 30. inf 30. inf inf inf inf 40.]\n",
            "[ 0. 30. inf 30. inf inf inf inf 40.]\n",
            "[[ 0. 30. inf 30. inf inf inf inf 40.]\n",
            " [30.  0. 25. 40. inf inf inf inf inf]\n",
            " [inf 25.  0. 50. inf inf inf inf inf]\n",
            " [30. 40. 50.  0. 30. 20. inf inf inf]\n",
            " [inf inf inf 30.  0. inf 25. inf inf]\n",
            " [inf inf inf 20. inf  0. 20. inf 20.]\n",
            " [inf inf inf inf 25. 20.  0. 25. inf]\n",
            " [inf inf inf inf inf inf 25.  0. 20.]\n",
            " [40. inf inf inf inf 20. inf 20.  0.]]\n",
            "Time:  0.28587139699993713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-iYUOfQfIJb",
        "colab_type": "text"
      },
      "source": [
        "#### **Floyd–Warshall distance** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jPmFIt2EfUC7"
      },
      "source": [
        "##### Construct distance matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc87o0r7fCTc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fcfc94f1-d21e-4d1f-8949-3e190421ea37"
      },
      "source": [
        "from timeit import default_timer\n",
        "from numba import cuda, njit\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def graph2dist(graph, dist_mtx, n):\n",
        "  stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "\n",
        "  ## initialize distance matrix\n",
        "  x, y = cuda.grid(2)\n",
        "  for i in range(x, n, stride):\n",
        "    for j in range(y, n, stride):\n",
        "      dist_mtx[i,j] = np.inf\n",
        "\n",
        "  ## calculate distance matrix\n",
        "  x = cuda.grid(1)\n",
        "  for i in range(x, graph.shape[0], stride):\n",
        "    a = int(graph[i,0]) - 1\n",
        "    b = int(graph[i,1]) - 1\n",
        "    d = graph[i,2]\n",
        "    dist_mtx[a,b] = d\n",
        "    dist_mtx[b,a] = d\n",
        "  \n",
        "  ## set diagonal to 0\n",
        "  y = cuda.grid(1)\n",
        "  if y < n:\n",
        "    dist_mtx[y,y] = 0.0\n",
        "\n",
        "\n",
        "def distance_matrix(graph, n):\n",
        "  ## copy data to device\n",
        "  graph_device = cuda.to_device(graph)\n",
        "  dist_mtx_device = cuda.device_array(shape=(n,n))\n",
        "\n",
        "  ## calculate distance matrix\n",
        "  graph2dist[dimGrid, dimBlock](graph_device, dist_mtx_device, n)\n",
        "  \n",
        "  ## copy data to host\n",
        "  dist_mtx_host = dist_mtx_device.copy_to_host()\n",
        " \n",
        "  return dist_mtx_host\n",
        "\n",
        "\n",
        "## print time costs\n",
        "try:\n",
        "  start = default_timer()\n",
        "  dist_mtx = distance_matrix(np.array(data), nodes)\n",
        "  stop = default_timer()\n",
        "  print('Time: ', stop - start)\n",
        "except:\n",
        "  print('Time: inf')\n",
        "  raise"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  0.30542288800006645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4SZOKJF6y6g",
        "colab_type": "text"
      },
      "source": [
        "##### Calculate Floyd–Warshall distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKiKJuWuy7qR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "580cc940-d03f-4141-aaa0-8bcbf1923872"
      },
      "source": [
        "from timeit import default_timer\n",
        "from numba import cuda, njit\n",
        "from operator import *\n",
        "import numpy as np\n",
        "\n",
        "@cuda.jit\n",
        "def all_pair_floyd(matrix, k, n):\n",
        "  stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "  x, y = cuda.grid(2)\n",
        "  if x < n and y < n:\n",
        "    matrix[x,y] = min(matrix[x,y], matrix[x,k] + matrix[k,y])\n",
        "  # for i in range(x, n, stride):\n",
        "  #   for j in range(y, n, stride):\n",
        "  #     matrix[i,j] = min(matrix[i,j], matrix[i,k] + matrix[k,j])\n",
        "\n",
        "\n",
        "def floyd_distance(matrix, n):\n",
        "  ## copy data to device\n",
        "  matrix_device = cuda.to_device(matrix)\n",
        "\n",
        "  ## calculate hedetniemi distance\n",
        "  for k in range(n):\n",
        "    all_pair_floyd[dimGrid, dimBlock](matrix_device, k, n)\n",
        "  \n",
        "  ## copy data to host\n",
        "  matrix_host = matrix_device.copy_to_host()\n",
        " \n",
        "  return matrix\n",
        "\n",
        "\n",
        "# print time costs\n",
        "try:\n",
        "  start = default_timer()\n",
        "  mtx_a_t = floyd_distance(dist_mtx, nodes)\n",
        "  stop = default_timer()\n",
        "  print('Time: ', stop - start)\n",
        "except:\n",
        "  print('Time: inf')\n",
        "  raise"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  0.31029173400020227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFUX5BF5nsS4",
        "colab_type": "text"
      },
      "source": [
        "#### **Compare results** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkRvkFrinryf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!diff 'hedet_mtx_list.txt' 'hedet_mtx_nb_cuda.txt'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "all_pair_distance_cuda.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMEQu0piMb7sgI9YVl71i/r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linyuehzzz/hedetniemi_distance/blob/master/all_pair_distance_cuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrTaIelAvUyh",
        "colab_type": "text"
      },
      "source": [
        "##**CUDA for all-pair distance algorithms**\n",
        "CUDA parallelism for all-pair distance algorithms.  \n",
        "Yue Lin (lin.3326 at osu.edu)  \n",
        "Created: 6/12/2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "083OUoAWPj2J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "60dd1748-7d30-4bba-eb21-46b7c7c704ca"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rya-4S2c_mPy"
      },
      "source": [
        "#### **Install packages** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7Au10ccQPam",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "91666832-8da8-4dda-d534-97a66801b790"
      },
      "source": [
        "!pip install timeout-decorator"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting timeout-decorator\n",
            "  Downloading https://files.pythonhosted.org/packages/07/1c/0d9adcb848f1690f3253dcb1c1557b6cf229a93e724977cb83f266cbd0ae/timeout-decorator-0.4.1.tar.gz\n",
            "Building wheels for collected packages: timeout-decorator\n",
            "  Building wheel for timeout-decorator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for timeout-decorator: filename=timeout_decorator-0.4.1-cp36-none-any.whl size=5021 sha256=9b0540b469469377d7ed885972b05ec464c65576633c1d30eab7aac4e80453f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/e6/ea/7387e3629cb46ba65140141f972745b823f4486c6fe884ccb8\n",
            "Successfully built timeout-decorator\n",
            "Installing collected packages: timeout-decorator\n",
            "Successfully installed timeout-decorator-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMGLlynLRyMO",
        "colab_type": "text"
      },
      "source": [
        "#### **CUDA device query** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oBHXLhsRfPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "c129ae00-2831-45d0-ad93-e2a18f151b0d"
      },
      "source": [
        "!nvcc --version\n",
        "from numba import cuda\n",
        "print(cuda.gpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "<Managed Device 0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnjDI4fbSGHm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "65fc794f-debe-49be-f637-cfaabc12072f"
      },
      "source": [
        "%cd /usr/local/cuda-10.1/samples/1_Utilities/deviceQuery\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/cuda-10.1/samples/1_Utilities/deviceQuery\n",
            "deviceQuery.cpp  Makefile  NsightEclipse.xml  readme.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOm7s7mBSRCg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "outputId": "73b29f3f-739a-4ba7-bed4-a3df018280b7"
      },
      "source": [
        "!make\n",
        "!./deviceQuery"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/cuda-10.1/bin/nvcc -ccbin g++ -I../../common/inc  -m64    -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_75,code=compute_75 -o deviceQuery.o -c deviceQuery.cpp\n",
            "/usr/local/cuda-10.1/bin/nvcc -ccbin g++   -m64      -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_75,code=compute_75 -o deviceQuery deviceQuery.o \n",
            "mkdir -p ../../bin/x86_64/linux/release\n",
            "cp deviceQuery ../../bin/x86_64/linux/release\n",
            "./deviceQuery Starting...\n",
            "\n",
            " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"Tesla P100-PCIE-16GB\"\n",
            "  CUDA Driver Version / Runtime Version          10.1 / 10.1\n",
            "  CUDA Capability Major/Minor version number:    6.0\n",
            "  Total amount of global memory:                 16281 MBytes (17071734784 bytes)\n",
            "  (56) Multiprocessors, ( 64) CUDA Cores/MP:     3584 CUDA Cores\n",
            "  GPU Max Clock rate:                            1329 MHz (1.33 GHz)\n",
            "  Memory Clock rate:                             715 Mhz\n",
            "  Memory Bus Width:                              4096-bit\n",
            "  L2 Cache Size:                                 4194304 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  2048\n",
            "  Maximum number of threads per block:           1024\n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated GPU sharing Host Memory:            No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Alignment requirement for Surfaces:            Yes\n",
            "  Device has ECC support:                        Enabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device supports Compute Preemption:            Yes\n",
            "  Supports Cooperative Kernel Launch:            Yes\n",
            "  Supports MultiDevice Co-op Kernel Launch:      Yes\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n",
            "  Compute Mode:\n",
            "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
            "\n",
            "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.1, NumDevs = 1\n",
            "Result = PASS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZhcIrCevlc4",
        "colab_type": "text"
      },
      "source": [
        "#### **Read graph data** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw69kvqGGpUL",
        "colab_type": "text"
      },
      "source": [
        "##### Data from the original article"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qev-12ZOGlco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## [node i, node j, distance between node i and j]\n",
        "## using data from example 1: San Francisco Bay Area Graph of Time-Distances (in minutes)\n",
        "data = [[1, 2, 30], [1, 4, 30], [1, 9, 40],\n",
        "        [2, 3, 25], [2, 4, 40], [3, 4, 50],\n",
        "        [4, 5, 30], [4, 6, 20], [5, 7, 25],\n",
        "        [6, 7, 20], [6, 9, 20], [7, 8, 25],\n",
        "        [8, 9, 20]]\n",
        "nodes = 9"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMaJB5Y1GmMQ",
        "colab_type": "text"
      },
      "source": [
        "##### Read random graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ej6mBJFQd3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "a26ca97d-3fb3-4fb1-f7c5-0adf392f479b"
      },
      "source": [
        "%cd '/content/gdrive/My Drive/Colab Notebooks/hedetniemi_matrix_sum'\n",
        "\n",
        "## Number of nodes (100/1,000/10,000/100,000/1,000,000)\n",
        "nodes = 1000\n",
        "print('Nodes: ', nodes)\n",
        "## Total degree\n",
        "degree = 3\n",
        "print('Degree: ', degree)\n",
        "\n",
        "data = []\n",
        "with open('graph_n' + str(nodes) + '_d' + str(degree) + '.txt', 'r') as f:\n",
        "  lines = f.read().splitlines()\n",
        "  for line in lines:\n",
        "    l = line.split()\n",
        "    item = [int(l[0]), int(l[1]), float(l[2])]\n",
        "    data.append(item)\n",
        "\n",
        "print(data[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/hedetniemi_matrix_sum\n",
            "Nodes:  1000\n",
            "Degree:  3\n",
            "[609, 621, 18.019071417527243]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cvAjdVsDR1FL"
      },
      "source": [
        "#### **Configure CUDA** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dpUmV2deRwDt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "816d1355-e1e0-4f77-c34a-65e99928a861"
      },
      "source": [
        "import math\n",
        "\n",
        "# number of streams\n",
        "NUM_STREAMS = 1\n",
        "# number of threads per block: 8, 16, 32\n",
        "NUM_THREADS = 3\n",
        "\n",
        "def get_cuda_execution_config(n):\n",
        "  numStream = NUM_STREAMS\n",
        "  numSegment = n // numStream\n",
        "  dimBlock = (NUM_THREADS, NUM_THREADS)\n",
        "  dimGrid = (math.ceil(numSegment / NUM_THREADS), math.ceil(numSegment / NUM_THREADS))\n",
        "\n",
        "  return dimGrid, dimBlock, numStream, numSegment\n",
        "\n",
        "\n",
        "dimGrid, dimBlock, numStream, numSegment = get_cuda_execution_config(nodes)\n",
        "print('numStream: ', numStream)\n",
        "print('numSegment: ', numSegment)\n",
        "print('dimGrid: ', dimGrid)\n",
        "print('dimBlock: ', dimBlock)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numStream:  1\n",
            "numSegment:  9\n",
            "dimGrid:  (3, 3)\n",
            "dimBlock:  (3, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8xMYn_Pyrk0",
        "colab_type": "text"
      },
      "source": [
        "#### **Hedetniemi distance** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93Y_nhjy8g8P",
        "colab_type": "text"
      },
      "source": [
        "##### Construct distance matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mX7kwBz8gPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f952be26-09e6-43f6-c16b-30e49dd7513a"
      },
      "source": [
        "from timeit import default_timer\n",
        "from numba import cuda, njit\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def graph2dist(graph, dist_mtx, n):\n",
        "  stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "\n",
        "  ## initialize distance matrix\n",
        "  x, y = cuda.grid(2)\n",
        "  if x < n and y < n:\n",
        "    dist_mtx[x,y] = np.inf\n",
        "\n",
        "  ## calculate distance matrix\n",
        "  for i in range(x, graph.shape[0], stride):\n",
        "    a = int(graph[i,0]) - 1\n",
        "    b = int(graph[i,1]) - 1\n",
        "    d = graph[i,2]\n",
        "    dist_mtx[a,b] = d\n",
        "    dist_mtx[b,a] = d\n",
        "  \n",
        "  ## set diagonal to 0\n",
        "  if x < n:\n",
        "    dist_mtx[x,x] = 0.0\n",
        "\n",
        "\n",
        "def distance_matrix(graph, n):\n",
        "  ## copy data to device\n",
        "  graph_device = cuda.to_device(graph)\n",
        "  dist_mtx_device = cuda.device_array(shape=(n,n))\n",
        "\n",
        "  ## calculate distance matrix\n",
        "  graph2dist[dimGrid, dimBlock](graph_device, dist_mtx_device, n)\n",
        "  \n",
        "  ## copy data to host\n",
        "  dist_mtx_host = dist_mtx_device.copy_to_host()\n",
        " \n",
        "  return dist_mtx_host\n",
        "\n",
        "\n",
        "## print time costs\n",
        "try:\n",
        "  start = default_timer()\n",
        "  dist_mtx = distance_matrix(np.array(data), nodes)\n",
        "  stop = default_timer()\n",
        "  print('Time: ', stop - start)\n",
        "except:\n",
        "  print('Time: inf')\n",
        "  raise"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  0.22052718199847732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_8aCYV6pFnX0"
      },
      "source": [
        "##### Calculate Hedetniemi Matrix Sum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwMScYG8LRSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "abd0df7f-4cf8-467a-b67d-d88701c78fb3"
      },
      "source": [
        "from timeit import default_timer\n",
        "from numba import cuda, njit, float32\n",
        "import numpy as np\n",
        "\n",
        "@cuda.jit\n",
        "def init_mtx(matrix, mtx_a_t_1, mtx_a_t, n):\n",
        "  # initialize distance matrix\n",
        "  x, y = cuda.grid(2)\n",
        "  if x < n and y < n:\n",
        "    mtx_a_t[x,y] = np.inf\n",
        "    mtx_a_t_1[x,y] = matrix[x,y]\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def all_pair_hedet(matrix, mtx_a_t_1, mtx_a_t, n, p):\n",
        "  x, y = cuda.grid(2)\n",
        "  if x < n and y < n:\n",
        "    summ = np.inf\n",
        "    for k in range(n):\n",
        "      summ = min(summ, mtx_a_t_1[x, k] + matrix[k, y])\n",
        "    mtx_a_t[x,y] = summ\n",
        "\n",
        "    if mtx_a_t_1[x,y] != mtx_a_t[x,y]:    \n",
        "      mtx_a_t_1[x,y] = mtx_a_t[x,y]\n",
        "      p[0] = False\n",
        "\n",
        "\n",
        "def hede_distance(matrix, n):\n",
        "  ## copy data to device\n",
        "  matrix_device = cuda.to_device(matrix)\n",
        "  mtx_a_t_1_device = cuda.device_array(shape=(n,n))\n",
        "  mtx_a_t_device = cuda.device_array(shape=(n,n))\n",
        "\n",
        "  ## initialize hedetniemi distance\n",
        "  init_mtx[dimGrid, dimBlock](matrix_device, mtx_a_t_1_device, mtx_a_t_device, n)\n",
        "\n",
        "  ## calculate hedetniemi distance\n",
        "  for k in range(n):\n",
        "    p = cuda.to_device([True])\n",
        "    all_pair_hedet[dimGrid, dimBlock](matrix_device, mtx_a_t_1_device, mtx_a_t_device, n, p)\n",
        "    if p[0] == True:\n",
        "      break\n",
        "  \n",
        "  ## copy data to host\n",
        "  mtx_a_t_host = mtx_a_t_device.copy_to_host()\n",
        " \n",
        "  return mtx_a_t_host\n",
        "\n",
        "\n",
        "## print time costs\n",
        "try:\n",
        "  start = default_timer()\n",
        "  mtx_a_t = hede_distance(dist_mtx, nodes)\n",
        "  stop = default_timer()\n",
        "  print('Time: ', stop - start)\n",
        "  print(mtx_a_t)\n",
        "except:\n",
        "  print('Time: inf')\n",
        "  raise\n",
        "\n",
        "## print shortest path matrix\n",
        "with open('hedet_mtx_nb_cuda.txt', 'w') as fw:\n",
        "  fw.write('\\n'.join(['\\t'.join([str(round(cell,2)) for cell in row]) for row in mtx_a_t.tolist()]))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  0.32872565599973314\n",
            "[[  0.  30.  55.  30.  60.  50.  70.  60.  40.]\n",
            " [ 30.   0.  25.  40.  70.  60.  80.  90.  70.]\n",
            " [ 55.  25.   0.  50.  80.  70.  90. 110.  90.]\n",
            " [ 30.  40.  50.   0.  30.  20.  40.  60.  40.]\n",
            " [ 60.  70.  80.  30.   0.  45.  25.  50.  65.]\n",
            " [ 50.  60.  70.  20.  45.   0.  20.  40.  20.]\n",
            " [ 70.  80.  90.  40.  25.  20.   0.  25.  40.]\n",
            " [ 60.  90. 110.  60.  50.  40.  25.   0.  20.]\n",
            " [ 40.  70.  90.  40.  65.  20.  40.  20.   0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5h8u3ftVA8W_"
      },
      "source": [
        "##### Calculate Hedetniemi Matrix Sum (shared memory)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lRZdMTXa0mU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "2d36cd31-e19d-4462-8ba8-84a628bfdcae"
      },
      "source": [
        "from timeit import default_timer\n",
        "from numba import cuda, njit, float32\n",
        "import numpy as np\n",
        "\n",
        "@cuda.jit\n",
        "def init_mtx(matrix, mtx_a_t_1, mtx_a_t, n):\n",
        "  # initialize distance matrix\n",
        "  x, y = cuda.grid(2)\n",
        "  if x < n and y < n:\n",
        "    mtx_a_t[x,y] = np.inf\n",
        "    mtx_a_t_1[x,y] = matrix[x,y]\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def all_pair_hedet(matrix, mtx_a_t_1, mtx_a_t, n, p):\n",
        "  # define an array in the shared memory\n",
        "  s_a = cuda.shared.array(shape=(NUM_THREADS, NUM_THREADS), dtype=float32)\n",
        "  s_b = cuda.shared.array(shape=(NUM_THREADS, NUM_THREADS), dtype=float32)\n",
        "\n",
        "  x, y = cuda.grid(2)\n",
        "\n",
        "  tx = cuda.threadIdx.x\n",
        "  ty = cuda.threadIdx.y\n",
        "\n",
        "  bpg = cuda.gridDim.x\n",
        "  tpb = cuda.blockDim.x\n",
        "\n",
        "  if x >= n and y >= n:\n",
        "    return\n",
        "  \n",
        "  # calculate matrix t\n",
        "  summ = np.inf\n",
        "  for i in range(bpg):\n",
        "    # preload data into shared memory\n",
        "    s_a[tx, ty] = mtx_a_t_1[x, ty + i * tpb]\n",
        "    s_b[tx, ty] = matrix[tx + i * tpb, y]\n",
        "    cuda.syncthreads()\n",
        "    for j in range(tpb):\n",
        "      summ = min(summ, s_a[tx, j] + s_b[j, ty])\n",
        "    cuda.syncthreads()\n",
        "  mtx_a_t[x,y] = summ\n",
        "\n",
        "  # compare matrix t and matrix t-1\n",
        "  if mtx_a_t_1[x,y] != mtx_a_t[x,y]:\n",
        "    mtx_a_t_1[x,y] = mtx_a_t[x,y]\n",
        "    p[0] = False\n",
        "\n",
        "\n",
        "def hede_distance(matrix, n):\n",
        "  ## copy data to device\n",
        "  matrix_device = cuda.to_device(matrix)\n",
        "  mtx_a_t_1_device = cuda.device_array(shape=(n,n))\n",
        "  mtx_a_t_device = cuda.device_array(shape=(n,n))\n",
        "\n",
        "  ## initialize hedetniemi distance\n",
        "  init_mtx[dimGrid, dimBlock](matrix_device, mtx_a_t_1_device, mtx_a_t_device, n)\n",
        "\n",
        "  ## calculate hedetniemi distance\n",
        "  for k in range(n):\n",
        "    p = cuda.to_device([True])\n",
        "    all_pair_hedet[dimGrid, dimBlock](matrix_device, mtx_a_t_1_device, mtx_a_t_device, n, p)\n",
        "    if p[0] == True:\n",
        "      break\n",
        "  \n",
        "  ## copy data to host\n",
        "  mtx_a_t_host = mtx_a_t_device.copy_to_host()\n",
        " \n",
        "  return mtx_a_t_host\n",
        "\n",
        "\n",
        "## print time costs\n",
        "try:\n",
        "  start = default_timer()\n",
        "  mtx_a_t = hede_distance(dist_mtx, nodes)\n",
        "  stop = default_timer()\n",
        "  print('Time: ', stop - start)\n",
        "  print(mtx_a_t)\n",
        "except:\n",
        "  print('Time: inf')\n",
        "  raise\n",
        "\n",
        "## print shortest path matrix\n",
        "with open('hedet_mtx_nb_cuda.txt', 'w') as fw:\n",
        "  fw.write('\\n'.join(['\\t'.join([str(round(cell,2)) for cell in row]) for row in mtx_a_t.tolist()]))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  0.3928115919998163\n",
            "[[  0.  30.  55.  30.  60.  50.  70.  60.  40.]\n",
            " [ 30.   0.  25.  40.  70.  60.  80.  90.  70.]\n",
            " [ 55.  25.   0.  50.  80.  70.  90. 110.  90.]\n",
            " [ 30.  40.  50.   0.  30.  20.  40.  60.  40.]\n",
            " [ 60.  70.  80.  30.   0.  45.  25.  50.  65.]\n",
            " [ 50.  60.  70.  20.  45.   0.  20.  40.  20.]\n",
            " [ 70.  80.  90.  40.  25.  20.   0.  25.  40.]\n",
            " [ 60.  90. 110.  60.  50.  40.  25.   0.  20.]\n",
            " [ 40.  70.  90.  40.  65.  20.  40.  20.   0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-iYUOfQfIJb",
        "colab_type": "text"
      },
      "source": [
        "#### **Floyd–Warshall distance** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jPmFIt2EfUC7"
      },
      "source": [
        "##### Construct distance matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc87o0r7fCTc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6648b52c-6f89-4440-afe3-95743d15c13d"
      },
      "source": [
        "from timeit import default_timer\n",
        "from numba import cuda, njit\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def graph2dist(graph, dist_mtx, n):\n",
        "  stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "\n",
        "  ## initialize distance matrix\n",
        "  x, y = cuda.grid(2)\n",
        "  if x < n and y < n:\n",
        "    dist_mtx[x,y] = np.inf\n",
        "\n",
        "  ## calculate distance matrix\n",
        "  for i in range(x, graph.shape[0], stride):\n",
        "    a = int(graph[i,0]) - 1\n",
        "    b = int(graph[i,1]) - 1\n",
        "    d = graph[i,2]\n",
        "    dist_mtx[a,b] = d\n",
        "    dist_mtx[b,a] = d\n",
        "  \n",
        "  ## set diagonal to 0\n",
        "  if x < n:\n",
        "    dist_mtx[x,x] = 0.0\n",
        "\n",
        "\n",
        "def distance_matrix(graph, n):\n",
        "  ## copy data to device\n",
        "  graph_device = cuda.to_device(graph)\n",
        "  dist_mtx_device = cuda.device_array(shape=(n,n))\n",
        "\n",
        "  ## calculate distance matrix\n",
        "  graph2dist[dimGrid, dimBlock](graph_device, dist_mtx_device, n)\n",
        "  \n",
        "  ## copy data to host\n",
        "  dist_mtx_host = dist_mtx_device.copy_to_host()\n",
        " \n",
        "  return dist_mtx_host\n",
        "\n",
        "\n",
        "## print time costs\n",
        "try:\n",
        "  start = default_timer()\n",
        "  dist_mtx = distance_matrix(np.array(data), nodes)\n",
        "  stop = default_timer()\n",
        "  print('Time: ', stop - start)\n",
        "except:\n",
        "  print('Time: inf')\n",
        "  raise"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  0.2746092460001819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4SZOKJF6y6g",
        "colab_type": "text"
      },
      "source": [
        "##### Calculate Floyd–Warshall distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKiKJuWuy7qR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9975377c-f1c6-4cce-e817-d9561acbaf71"
      },
      "source": [
        "from timeit import default_timer\n",
        "from numba import cuda, njit\n",
        "from operator import *\n",
        "import numpy as np\n",
        "\n",
        "@cuda.jit\n",
        "def all_pair_floyd(matrix, k, n):\n",
        "  x, y = cuda.grid(2)\n",
        "  if x < n and y < n:\n",
        "    matrix[x,y] = min(matrix[x,y], matrix[x,k] + matrix[k,y])\n",
        "\n",
        "\n",
        "def floyd_distance(matrix, n):\n",
        "  ## copy data to device\n",
        "  matrix_device = cuda.to_device(matrix)\n",
        "\n",
        "  ## calculate hedetniemi distance\n",
        "  for k in range(n):\n",
        "    all_pair_floyd[dimGrid, dimBlock](matrix_device, k, n)\n",
        "  \n",
        "  ## copy data to host\n",
        "  matrix_host = matrix_device.copy_to_host()\n",
        " \n",
        "  return matrix_host\n",
        "\n",
        "\n",
        "# print time costs\n",
        "try:\n",
        "  start = default_timer()\n",
        "  mtx_a_t = floyd_distance(dist_mtx, nodes)\n",
        "  stop = default_timer()\n",
        "  print('Time: ', stop - start)\n",
        "except:\n",
        "  print('Time: inf')\n",
        "  raise\n",
        "\n",
        "## print shortest path matrix\n",
        "with open('floyd_mtx_nb_cuda.txt', 'w') as fw:\n",
        "  fw.write('\\n'.join(['\\t'.join([str(round(cell,2)) for cell in row]) for row in mtx_a_t.tolist()]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  0.27073387800191995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFUX5BF5nsS4",
        "colab_type": "text"
      },
      "source": [
        "#### **Compare results** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkRvkFrinryf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!diff 'hedet_mtx_list.txt' 'hedet_mtx_nb_cuda.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4xw-8e7N-b7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!diff 'hedet_mtx_nb_cuda.txt' 'floyd_mtx_nb_cuda.txt'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "all_pair_distance_cuda.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOWfljSlitPeZorxNlqtxl0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linyuehzzz/hedetniemi_distance/blob/master/all_pair_distance_cuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrTaIelAvUyh",
        "colab_type": "text"
      },
      "source": [
        "##**CUDA for all-pair distance algorithms**\n",
        "CUDA parallelism for all-pair distance algorithms.  \n",
        "Yue Lin (lin.3326 at osu.edu)  \n",
        "Created: 6/12/2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "083OUoAWPj2J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "1b80d01e-af3a-4523-8c0b-e3c37233804f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rya-4S2c_mPy"
      },
      "source": [
        "#### **Install packages** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7Au10ccQPam",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "3930c727-8c22-42b6-9c72-905a3583da8f"
      },
      "source": [
        "!pip install timeout-decorator"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting timeout-decorator\n",
            "  Downloading https://files.pythonhosted.org/packages/07/1c/0d9adcb848f1690f3253dcb1c1557b6cf229a93e724977cb83f266cbd0ae/timeout-decorator-0.4.1.tar.gz\n",
            "Building wheels for collected packages: timeout-decorator\n",
            "  Building wheel for timeout-decorator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for timeout-decorator: filename=timeout_decorator-0.4.1-cp36-none-any.whl size=5021 sha256=50187796e4147829c0e7a0151e4055070f440677bdfcdd37bebcad9eeb8e8be9\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/e6/ea/7387e3629cb46ba65140141f972745b823f4486c6fe884ccb8\n",
            "Successfully built timeout-decorator\n",
            "Installing collected packages: timeout-decorator\n",
            "Successfully installed timeout-decorator-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMGLlynLRyMO",
        "colab_type": "text"
      },
      "source": [
        "#### **CUDA device query** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oBHXLhsRfPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "c129ae00-2831-45d0-ad93-e2a18f151b0d"
      },
      "source": [
        "!nvcc --version\n",
        "from numba import cuda\n",
        "print(cuda.gpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "<Managed Device 0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnjDI4fbSGHm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cfce08ae-fbf0-41fa-a635-57d4f2e083d2"
      },
      "source": [
        "%cd /usr/local/cuda-10.1/samples/1_Utilities/deviceQuery\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/cuda-10.1/samples/1_Utilities/deviceQuery\n",
            "deviceQuery.cpp  Makefile  NsightEclipse.xml  readme.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOm7s7mBSRCg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "outputId": "d59f0379-58f6-45db-b73e-f0f9005556aa"
      },
      "source": [
        "!make\n",
        "!./deviceQuery"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make: Nothing to be done for 'all'.\n",
            "./deviceQuery Starting...\n",
            "\n",
            " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"Tesla P100-PCIE-16GB\"\n",
            "  CUDA Driver Version / Runtime Version          10.1 / 10.1\n",
            "  CUDA Capability Major/Minor version number:    6.0\n",
            "  Total amount of global memory:                 16281 MBytes (17071734784 bytes)\n",
            "  (56) Multiprocessors, ( 64) CUDA Cores/MP:     3584 CUDA Cores\n",
            "  GPU Max Clock rate:                            1329 MHz (1.33 GHz)\n",
            "  Memory Clock rate:                             715 Mhz\n",
            "  Memory Bus Width:                              4096-bit\n",
            "  L2 Cache Size:                                 4194304 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  2048\n",
            "  Maximum number of threads per block:           1024\n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated GPU sharing Host Memory:            No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Alignment requirement for Surfaces:            Yes\n",
            "  Device has ECC support:                        Enabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device supports Compute Preemption:            Yes\n",
            "  Supports Cooperative Kernel Launch:            Yes\n",
            "  Supports MultiDevice Co-op Kernel Launch:      Yes\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n",
            "  Compute Mode:\n",
            "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
            "\n",
            "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.1, NumDevs = 1\n",
            "Result = PASS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZhcIrCevlc4",
        "colab_type": "text"
      },
      "source": [
        "#### **Read graph data** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw69kvqGGpUL",
        "colab_type": "text"
      },
      "source": [
        "##### Data from the original article"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qev-12ZOGlco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## [node i, node j, distance between node i and j]\n",
        "## using data from example 1: San Francisco Bay Area Graph of Time-Distances (in minutes)\n",
        "data = [[1, 2, 30], [1, 4, 30], [1, 9, 40],\n",
        "        [2, 3, 25], [2, 4, 40], [3, 4, 50],\n",
        "        [4, 5, 30], [4, 6, 20], [5, 7, 25],\n",
        "        [6, 7, 20], [6, 9, 20], [7, 8, 25],\n",
        "        [8, 9, 20]]\n",
        "nodes = 9"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMaJB5Y1GmMQ",
        "colab_type": "text"
      },
      "source": [
        "##### Read random graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ej6mBJFQd3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "49fd1402-be8c-4a7a-fe99-4998b8e40f8c"
      },
      "source": [
        "%cd '/content/gdrive/My Drive/Colab Notebooks/hedetniemi_matrix_sum'\n",
        "\n",
        "## Number of nodes (100/1,000/10,000/100,000/1,000,000)\n",
        "nodes = 100\n",
        "print('Nodes: ', nodes)\n",
        "## Total degree\n",
        "degree = 3\n",
        "print('Degree: ', degree)\n",
        "\n",
        "data = []\n",
        "with open('graph_n' + str(nodes) + '_d' + str(degree) + '.txt', 'r') as f:\n",
        "  lines = f.read().splitlines()\n",
        "  for line in lines:\n",
        "    l = line.split()\n",
        "    item = [int(l[0]), int(l[1]), float(l[2])]\n",
        "    data.append(item)\n",
        "\n",
        "print(data[0])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/hedetniemi_matrix_sum\n",
            "Nodes:  100\n",
            "Degree:  3\n",
            "[77, 86, 89.39726376738572]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cvAjdVsDR1FL"
      },
      "source": [
        "#### **Configure CUDA** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dpUmV2deRwDt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "3bc8809d-61cd-409a-e973-f77c84661f02"
      },
      "source": [
        "import math\n",
        "\n",
        "# number of streams\n",
        "NUM_STREAMS = 1\n",
        "# number of threads per block: 32、128、256\n",
        "NUM_THREADS = 32\n",
        "\n",
        "def get_cuda_execution_config(n):\n",
        "  numStream = NUM_STREAMS\n",
        "  numSegment = n // numStream\n",
        "  dimBlock = (NUM_THREADS, NUM_THREADS)\n",
        "  dimGrid = (math.ceil(numSegment / NUM_THREADS), math.ceil(numSegment / NUM_THREADS))\n",
        "\n",
        "  return dimGrid, dimBlock, numStream, numSegment\n",
        "\n",
        "\n",
        "dimGrid, dimBlock, numStream, numSegment = get_cuda_execution_config(nodes)\n",
        "print('numStream: ', numStream)\n",
        "print('numSegment: ', numSegment)\n",
        "print('dimGrid: ', dimGrid)\n",
        "print('dimBlock: ', dimBlock)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numStream:  1\n",
            "numSegment:  100\n",
            "dimGrid:  (4, 4)\n",
            "dimBlock:  (32, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8xMYn_Pyrk0",
        "colab_type": "text"
      },
      "source": [
        "#### **Hedetniemi distance** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93Y_nhjy8g8P",
        "colab_type": "text"
      },
      "source": [
        "##### Construct distance matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mX7kwBz8gPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1e46c87b-117e-47bb-97e6-ca2a213719cd"
      },
      "source": [
        "from timeit import default_timer\n",
        "from numba import cuda, njit\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def graph2dist(graph, dist_mtx, n):\n",
        "  stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "\n",
        "  ## initialize distance matrix\n",
        "  x, y = cuda.grid(2)\n",
        "  for i in range(x, n, stride):\n",
        "    for j in range(y, n, stride):\n",
        "      dist_mtx[i,j] = np.inf\n",
        "\n",
        "  ## calculate distance matrix\n",
        "  x = cuda.grid(1)\n",
        "  for i in range(x, graph.shape[0], stride):\n",
        "    a = int(graph[i,0]) - 1\n",
        "    b = int(graph[i,1]) - 1\n",
        "    d = graph[i,2]\n",
        "    dist_mtx[a,b] = d\n",
        "    dist_mtx[b,a] = d\n",
        "  \n",
        "  ## set diagonal to 0\n",
        "  y = cuda.grid(1)\n",
        "  if y < n:\n",
        "    dist_mtx[y,y] = 0.0\n",
        "\n",
        "\n",
        "def distance_matrix(graph, n):\n",
        "  ## copy data to device\n",
        "  graph_device = cuda.to_device(graph)\n",
        "  dist_mtx_device = cuda.device_array(shape=(n,n))\n",
        "\n",
        "  ## calculate distance matrix\n",
        "  graph2dist[dimGrid, dimBlock](graph_device, dist_mtx_device, n)\n",
        "  \n",
        "  ## copy data to host\n",
        "  dist_mtx_host = dist_mtx_device.copy_to_host()\n",
        " \n",
        "  return dist_mtx_host\n",
        "\n",
        "\n",
        "## print time costs\n",
        "try:\n",
        "  start = default_timer()\n",
        "  dist_mtx = distance_matrix(np.array(data), nodes)\n",
        "  stop = default_timer()\n",
        "  print('Time: ', stop - start)\n",
        "except:\n",
        "  print('Time: inf')\n",
        "  raise"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  0.43958123799984605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_8aCYV6pFnX0"
      },
      "source": [
        "##### Calculate Hedetniemi Matrix Sum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwMScYG8LRSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "17f7a0f9-5ea0-4d63-e03b-9aac1de5fc5f"
      },
      "source": [
        "from timeit import default_timer\n",
        "from numba import cuda, njit, float32\n",
        "from operator import *\n",
        "import numpy as np\n",
        "\n",
        "@cuda.jit\n",
        "def init_mtx(matrix, mtx_a_t_1, mtx_a_t, n):\n",
        "  # initialize distance matrix\n",
        "  x, y = cuda.grid(2)\n",
        "  if x < n and y < n:\n",
        "    mtx_a_t[x,y] = np.inf\n",
        "    mtx_a_t_1[x,y] = matrix[x,y]\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def all_pair_hedet(matrix, mtx_a_t_1, mtx_a_t, n):\n",
        "  x, y = cuda.grid(2)\n",
        "  if x < n and y < n:\n",
        "    summ = np.inf\n",
        "    for k in range(n):\n",
        "      summ = min(summ, mtx_a_t_1[x, k] + matrix[k, y])\n",
        "    mtx_a_t[x,y] = summ\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def cmp_mtx(mtx_a_t_1, mtx_a_t, n, p):\n",
        "  x, y = cuda.grid(2)\n",
        "  if x < n and y < n and p == True:\n",
        "    if mtx_a_t_1[x,y] != mtx_a_t[x,y]:\n",
        "      p = False\n",
        "  \n",
        "  if p == False:\n",
        "    x, y = cuda.grid(2)\n",
        "    if x < n and y < n:\n",
        "      mtx_a_t_1[x,y] = mtx_a_t[x,y]\n",
        "\n",
        "\n",
        "def hede_distance(matrix, n):\n",
        "  ## copy data to device\n",
        "  matrix_device = cuda.to_device(matrix)\n",
        "  mtx_a_t_1_device = cuda.device_array(shape=(n,n))\n",
        "  mtx_a_t_device = cuda.device_array(shape=(n,n))\n",
        "\n",
        "  ## initialize hedetniemi distance\n",
        "  init_mtx[dimGrid, dimBlock](matrix_device, mtx_a_t_1_device, mtx_a_t_device, n)\n",
        "\n",
        "  ## calculate hedetniemi distance\n",
        "  p = False\n",
        "  for k in range(n):\n",
        "    all_pair_hedet[dimGrid, dimBlock](matrix_device, mtx_a_t_1_device, mtx_a_t_device, n)\n",
        "    cmp_mtx[dimGrid, dimBlock](mtx_a_t_1_device, mtx_a_t_device, n, p)\n",
        "    if p == True:\n",
        "      break\n",
        "  \n",
        "  ## copy data to host\n",
        "  mtx_a_t_host = mtx_a_t_device.copy_to_host()\n",
        " \n",
        "  return mtx_a_t_host\n",
        "\n",
        "\n",
        "## print time costs\n",
        "try:\n",
        "  start = default_timer()\n",
        "  mtx_a_t = hede_distance(dist_mtx, nodes)\n",
        "  stop = default_timer()\n",
        "  print('Time: ', stop - start)\n",
        "except:\n",
        "  print('Time: inf')\n",
        "  raise\n",
        "\n",
        "## print shortest path matrix\n",
        "with open('hedet_mtx_nb_cuda.txt', 'w') as fw:\n",
        "  fw.write('\\n'.join(['\\t'.join([str(round(cell,2)) for cell in row]) for row in mtx_a_t.tolist()]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  0.6003975319999881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-iYUOfQfIJb",
        "colab_type": "text"
      },
      "source": [
        "#### **Floyd–Warshall distance** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jPmFIt2EfUC7"
      },
      "source": [
        "##### Construct distance matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc87o0r7fCTc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7e0be615-de5a-4b77-dd94-a7f13080ec3b"
      },
      "source": [
        "from timeit import default_timer\n",
        "from numba import cuda, njit\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def graph2dist(graph, dist_mtx, n):\n",
        "  stride = cuda.gridDim.x * cuda.blockDim.x\n",
        "\n",
        "  ## initialize distance matrix\n",
        "  x, y = cuda.grid(2)\n",
        "  for i in range(x, n, stride):\n",
        "    for j in range(y, n, stride):\n",
        "      dist_mtx[i,j] = np.inf\n",
        "\n",
        "  ## calculate distance matrix\n",
        "  x = cuda.grid(1)\n",
        "  for i in range(x, graph.shape[0], stride):\n",
        "    a = int(graph[i,0]) - 1\n",
        "    b = int(graph[i,1]) - 1\n",
        "    d = graph[i,2]\n",
        "    dist_mtx[a,b] = d\n",
        "    dist_mtx[b,a] = d\n",
        "  \n",
        "  ## set diagonal to 0\n",
        "  y = cuda.grid(1)\n",
        "  if y < n:\n",
        "    dist_mtx[y,y] = 0.0\n",
        "\n",
        "\n",
        "def distance_matrix(graph, n):\n",
        "  ## copy data to device\n",
        "  graph_device = cuda.to_device(graph)\n",
        "  dist_mtx_device = cuda.device_array(shape=(n,n))\n",
        "\n",
        "  ## calculate distance matrix\n",
        "  graph2dist[dimGrid, dimBlock](graph_device, dist_mtx_device, n)\n",
        "  \n",
        "  ## copy data to host\n",
        "  dist_mtx_host = dist_mtx_device.copy_to_host()\n",
        " \n",
        "  return dist_mtx_host\n",
        "\n",
        "\n",
        "## print time costs\n",
        "try:\n",
        "  start = default_timer()\n",
        "  dist_mtx = distance_matrix(np.array(data), nodes)\n",
        "  stop = default_timer()\n",
        "  print('Time: ', stop - start)\n",
        "except:\n",
        "  print('Time: inf')\n",
        "  raise"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  0.3656711849998828\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4SZOKJF6y6g",
        "colab_type": "text"
      },
      "source": [
        "##### Calculate Floyd–Warshall distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKiKJuWuy7qR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5c2319fb-5101-42cd-cfae-6c38ded12bf0"
      },
      "source": [
        "from timeit import default_timer\n",
        "from numba import cuda, njit\n",
        "from operator import *\n",
        "import numpy as np\n",
        "\n",
        "@cuda.jit\n",
        "def all_pair_floyd(matrix, k, n):\n",
        "  x, y = cuda.grid(2)\n",
        "  if x < n and y < n:\n",
        "    matrix[x,y] = min(matrix[x,y], matrix[x,k] + matrix[k,y])\n",
        "\n",
        "\n",
        "def floyd_distance(matrix, n):\n",
        "  ## copy data to device\n",
        "  matrix_device = cuda.to_device(matrix)\n",
        "\n",
        "  ## calculate hedetniemi distance\n",
        "  for k in range(n):\n",
        "    all_pair_floyd[dimGrid, dimBlock](matrix_device, k, n)\n",
        "  \n",
        "  ## copy data to host\n",
        "  matrix_host = matrix_device.copy_to_host()\n",
        " \n",
        "  return matrix_host\n",
        "\n",
        "\n",
        "# print time costs\n",
        "try:\n",
        "  start = default_timer()\n",
        "  mtx_a_t = floyd_distance(dist_mtx, nodes)\n",
        "  stop = default_timer()\n",
        "  print('Time: ', stop - start)\n",
        "except:\n",
        "  print('Time: inf')\n",
        "  raise\n",
        "\n",
        "## print shortest path matrix\n",
        "with open('floyd_mtx_nb_cuda.txt', 'w') as fw:\n",
        "  fw.write('\\n'.join(['\\t'.join([str(round(cell,2)) for cell in row]) for row in mtx_a_t.tolist()]))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  0.18667717700009234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFUX5BF5nsS4",
        "colab_type": "text"
      },
      "source": [
        "#### **Compare results** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkRvkFrinryf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!diff 'hedet_mtx_list.txt' 'hedet_mtx_nb_cuda.txt'"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4xw-8e7N-b7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!diff 'floyd_mtx_list.txt' 'floyd_mtx_nb_cuda.txt'"
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}